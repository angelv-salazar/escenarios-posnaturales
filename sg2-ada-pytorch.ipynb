{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"sg2-ada-pytorch.ipynb","provenance":[{"file_id":"https://github.com/angelv-salazar/escenarios-posnaturales/blob/main/sg2_ada_pytorch.ipynb","timestamp":1637159953344}],"collapsed_sections":["GwPrEVh5coPf"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qZkTQZc7cMWN"},"source":["# Entrenamiento personalizado: StyleGan2-ADA\n","\n","En este cuaderno, realizaremos el aprendizaje por transferencia con StyleGAN2 y conjuntos de datos personalizados.\n","\n","Esto significa que no entrenaremos a la red GAN con nuestras imágenes desde cero (ya que toma alrededor de dos semanas) pero usaremos el modelo ya entrenado en las otras imágenes como punto de partida. Reducirá el tiempo de entrenamiento a aproximadamente 10 horas al omitir las primeras etapas donde la red neuronal aprende características de bajo nivel de imágenes que son casi las mismas para cualquier tipo de imágenes."]},{"cell_type":"code","metadata":{"cellView":"form","id":"WOgc3AOiY6iU"},"source":["#@title Montar Google Drive\n","#@markdown Monte Google Drive para cargar las imágens de las bases de datos o los modelos previamente entrenados y guardar los resultados.\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-9CrUIl4bTxr"},"source":["drive.mount(\"/content/drive\", force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"G6nP8w7IZDpb"},"source":["#@title Instalación\n","#@markdown StyleGAN2-ADA será instalada en su Google Drive para acelerar el proceso de entrenamiento.\n","\n","#@markdown Ejecute esta celda. Si ya ha instalado el repositorio, se omitirá el proceso de instalación y actualizará el directorio del repositorio. Si no lo ha instalado, instalará todos los archivos necesarios.\n","import os\n","if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n","    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n","elif os.path.isdir(\"/content/drive/\"):\n","    #install script\n","    %cd \"/content/drive/MyDrive/\"\n","    !mkdir colab-sg2-ada-pytorch\n","    %cd colab-sg2-ada-pytorch\n","    !git clone https://github.com/angelv-salazar/stylegan2-ada-pytorch\n","    %cd stylegan2-ada-pytorch\n","    !mkdir downloads\n","    !mkdir datasets\n","    !mkdir pretrained\n","    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n","else:\n","    !git clone https://github.com/angelv-salazar/stylegan2-ada-pytorch\n","    %cd stylegan2-ada-pytorch\n","    !mkdir downloads\n","    !mkdir datasets\n","    !mkdir pretrained\n","    %cd pretrained\n","    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n","    %cd ../\n","\n","%tensorflow_version 1.x\n","!pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install ninja opensimplex\n","\n","%cd \"/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n","!git config --global user.name \"test\"\n","!git config --global user.email \"test@test.com\"\n","!git fetch origin\n","!git pull\n","!git stash\n","!git checkout origin/main -- train.py generate.py legacy.py closed_form_factorization.py flesh_digression.py apply_factor.py README.md calc_metrics.py training/stylegan2_multi.py training/training_loop.py util/utilgan.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"-MgOcCseZqlA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658343304185,"user_tz":180,"elapsed":8232,"user":{"displayName":"Angel Salazar","userId":"10004265032665965397"}},"outputId":"8b1dc9b4-e66d-4f6c-fd7b-6faf1e247aaf"},"source":["#@title Preparación de Datos\n","#@markdown Directorio de entrada de Imágenes\n","input_dir = '/content/drive/MyDrive/images-512' #@param {type: \"string\"}\n","#@markdown Ruta al archivo *zip* donde se almacenará el *Dataset* convertido. Usted debe crearlo la primera vez.\n","dataset_file = '/content/drive/MyDrive/images-512.zip' #@param {type: \"string\"}\n","\n","if not dataset_file.endswith('.zip'):\n","  dataset_file += '.zip'\n","\n","!python dataset_tool.py --source {input_dir} --dest {dataset_file}"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 158/158 [00:07<00:00, 20.77it/s]\n"]}]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"E25JTmDbZX1z","outputId":"ebcfe815-5ae8-42df-cd9b-a3958235e6fa"},"source":["#@title Entrenamiento de Modelo Personalizado\n","\n","#@markdown Ruta al archivo *zip* del *Dataset*\n","dataset = \"/content/drive/MyDrive/images-512.zip\" #@param {type: \"string\"}\n","\n","#@markdown Para el aprendizaje por transferencia, configúrelo en **ffhq256**, **ffhq512** o **ffhq1024** de acuerdo con la resolución de sus imágenes.\n","#@markdown Si desea reanudar el proceso de capacitación, proporcione la ruta a su último archivo *.pkl*\n","resume_from = \"ffhq512\" #@param {type: \"string\"}\n","\n","#don't edit this unless you know what you're doing :)\n","!python train.py --outdir ./results --snap=2 --cfg='11gb-gpu' --data={dataset} --aug=noaug --mirror=False --mirrory=False --metrics=None --resume={resume_from}"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Training options:\n","{\n","  \"num_gpus\": 1,\n","  \"image_snapshot_ticks\": 2,\n","  \"network_snapshot_ticks\": 2,\n","  \"metrics\": [],\n","  \"random_seed\": 0,\n","  \"training_set_kwargs\": {\n","    \"class_name\": \"training.dataset.ImageFolderDataset\",\n","    \"path\": \"/content/drive/MyDrive/images-512.zip\",\n","    \"use_labels\": false,\n","    \"max_size\": 158,\n","    \"xflip\": false,\n","    \"resolution\": 512\n","  },\n","  \"data_loader_kwargs\": {\n","    \"pin_memory\": true,\n","    \"num_workers\": 3,\n","    \"prefetch_factor\": 2\n","  },\n","  \"G_kwargs\": {\n","    \"class_name\": \"training.networks.Generator\",\n","    \"z_dim\": 512,\n","    \"w_dim\": 512,\n","    \"mapping_kwargs\": {\n","      \"num_layers\": 8\n","    },\n","    \"synthesis_kwargs\": {\n","      \"channel_base\": 32768,\n","      \"channel_max\": 512,\n","      \"num_fp16_res\": 4,\n","      \"conv_clamp\": 256\n","    }\n","  },\n","  \"D_kwargs\": {\n","    \"class_name\": \"training.networks.Discriminator\",\n","    \"block_kwargs\": {},\n","    \"mapping_kwargs\": {},\n","    \"epilogue_kwargs\": {\n","      \"mbstd_group_size\": 4\n","    },\n","    \"channel_base\": 32768,\n","    \"channel_max\": 512,\n","    \"num_fp16_res\": 4,\n","    \"conv_clamp\": 256\n","  },\n","  \"G_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.002,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"D_opt_kwargs\": {\n","    \"class_name\": \"torch.optim.Adam\",\n","    \"lr\": 0.002,\n","    \"betas\": [\n","      0,\n","      0.99\n","    ],\n","    \"eps\": 1e-08\n","  },\n","  \"loss_kwargs\": {\n","    \"class_name\": \"training.loss.StyleGAN2Loss\",\n","    \"r1_gamma\": 10\n","  },\n","  \"total_kimg\": 25000,\n","  \"batch_size\": 4,\n","  \"batch_gpu\": 4,\n","  \"ema_kimg\": 10,\n","  \"ema_rampup\": null,\n","  \"resume_pkl\": \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res512-mirror-stylegan2-noaug.pkl\",\n","  \"ada_kimg\": 100,\n","  \"run_dir\": \"./results/00049-images-512-11gb-gpu-noaug-resumeffhq512\"\n","}\n","\n","Output directory:   ./results/00049-images-512-11gb-gpu-noaug-resumeffhq512\n","Training data:      /content/drive/MyDrive/images-512.zip\n","Training duration:  25000 kimg\n","Number of GPUs:     1\n","Number of images:   158\n","Image resolution:   512\n","Conditional model:  False\n","Dataset x-flips:    False\n","\n","Creating output directory...\n","Launching processes...\n","Loading training set...\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","\n","Num images:  158\n","Image shape: [3, 512, 512]\n","Label shape: [0]\n","\n","Constructing networks...\n","starting G epochs:  0.0\n","Resuming from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res512-mirror-stylegan2-noaug.pkl\"\n","Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res512-mirror-stylegan2-noaug.pkl ... done\n","Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n","Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n","\n","Generator             Parameters  Buffers  Output shape        Datatype\n","---                   ---         ---      ---                 ---     \n","mapping.fc0           262656      -        [4, 512]            float32 \n","mapping.fc1           262656      -        [4, 512]            float32 \n","mapping.fc2           262656      -        [4, 512]            float32 \n","mapping.fc3           262656      -        [4, 512]            float32 \n","mapping.fc4           262656      -        [4, 512]            float32 \n","mapping.fc5           262656      -        [4, 512]            float32 \n","mapping.fc6           262656      -        [4, 512]            float32 \n","mapping.fc7           262656      -        [4, 512]            float32 \n","mapping               -           512      [4, 16, 512]        float32 \n","synthesis.b4.conv1    2622465     32       [4, 512, 4, 4]      float32 \n","synthesis.b4.torgb    264195      -        [4, 3, 4, 4]        float32 \n","synthesis.b4:0        8192        16       [4, 512, 4, 4]      float32 \n","synthesis.b4:1        -           -        [4, 512, 4, 4]      float32 \n","synthesis.b8.conv0    2622465     80       [4, 512, 8, 8]      float32 \n","synthesis.b8.conv1    2622465     80       [4, 512, 8, 8]      float32 \n","synthesis.b8.torgb    264195      -        [4, 3, 8, 8]        float32 \n","synthesis.b8:0        -           16       [4, 512, 8, 8]      float32 \n","synthesis.b8:1        -           -        [4, 512, 8, 8]      float32 \n","synthesis.b16.conv0   2622465     272      [4, 512, 16, 16]    float32 \n","synthesis.b16.conv1   2622465     272      [4, 512, 16, 16]    float32 \n","synthesis.b16.torgb   264195      -        [4, 3, 16, 16]      float32 \n","synthesis.b16:0       -           16       [4, 512, 16, 16]    float32 \n","synthesis.b16:1       -           -        [4, 512, 16, 16]    float32 \n","synthesis.b32.conv0   2622465     1040     [4, 512, 32, 32]    float32 \n","synthesis.b32.conv1   2622465     1040     [4, 512, 32, 32]    float32 \n","synthesis.b32.torgb   264195      -        [4, 3, 32, 32]      float32 \n","synthesis.b32:0       -           16       [4, 512, 32, 32]    float32 \n","synthesis.b32:1       -           -        [4, 512, 32, 32]    float32 \n","synthesis.b64.conv0   2622465     4112     [4, 512, 64, 64]    float16 \n","synthesis.b64.conv1   2622465     4112     [4, 512, 64, 64]    float16 \n","synthesis.b64.torgb   264195      -        [4, 3, 64, 64]      float16 \n","synthesis.b64:0       -           16       [4, 512, 64, 64]    float16 \n","synthesis.b64:1       -           -        [4, 512, 64, 64]    float32 \n","synthesis.b128.conv0  1442561     16400    [4, 256, 128, 128]  float16 \n","synthesis.b128.conv1  721409      16400    [4, 256, 128, 128]  float16 \n","synthesis.b128.torgb  132099      -        [4, 3, 128, 128]    float16 \n","synthesis.b128:0      -           16       [4, 256, 128, 128]  float16 \n","synthesis.b128:1      -           -        [4, 256, 128, 128]  float32 \n","synthesis.b256.conv0  426369      65552    [4, 128, 256, 256]  float16 \n","synthesis.b256.conv1  213249      65552    [4, 128, 256, 256]  float16 \n","synthesis.b256.torgb  66051       -        [4, 3, 256, 256]    float16 \n","synthesis.b256:0      -           16       [4, 128, 256, 256]  float16 \n","synthesis.b256:1      -           -        [4, 128, 256, 256]  float32 \n","synthesis.b512.conv0  139457      262160   [4, 64, 512, 512]   float16 \n","synthesis.b512.conv1  69761       262160   [4, 64, 512, 512]   float16 \n","synthesis.b512.torgb  33027       -        [4, 3, 512, 512]    float16 \n","synthesis.b512:0      -           16       [4, 64, 512, 512]   float16 \n","synthesis.b512:1      -           -        [4, 64, 512, 512]   float32 \n","---                   ---         ---      ---                 ---     \n","Total                 30276583    699904   -                   -       \n","\n","\n","Discriminator  Parameters  Buffers  Output shape        Datatype\n","---            ---         ---      ---                 ---     \n","b512.fromrgb   256         16       [4, 64, 512, 512]   float16 \n","b512.skip      8192        16       [4, 128, 256, 256]  float16 \n","b512.conv0     36928       16       [4, 64, 512, 512]   float16 \n","b512.conv1     73856       16       [4, 128, 256, 256]  float16 \n","b512           -           16       [4, 128, 256, 256]  float16 \n","b256.skip      32768       16       [4, 256, 128, 128]  float16 \n","b256.conv0     147584      16       [4, 128, 256, 256]  float16 \n","b256.conv1     295168      16       [4, 256, 128, 128]  float16 \n","b256           -           16       [4, 256, 128, 128]  float16 \n","b128.skip      131072      16       [4, 512, 64, 64]    float16 \n","b128.conv0     590080      16       [4, 256, 128, 128]  float16 \n","b128.conv1     1180160     16       [4, 512, 64, 64]    float16 \n","b128           -           16       [4, 512, 64, 64]    float16 \n","b64.skip       262144      16       [4, 512, 32, 32]    float16 \n","b64.conv0      2359808     16       [4, 512, 64, 64]    float16 \n","b64.conv1      2359808     16       [4, 512, 32, 32]    float16 \n","b64            -           16       [4, 512, 32, 32]    float16 \n","b32.skip       262144      16       [4, 512, 16, 16]    float32 \n","b32.conv0      2359808     16       [4, 512, 32, 32]    float32 \n","b32.conv1      2359808     16       [4, 512, 16, 16]    float32 \n","b32            -           16       [4, 512, 16, 16]    float32 \n","b16.skip       262144      16       [4, 512, 8, 8]      float32 \n","b16.conv0      2359808     16       [4, 512, 16, 16]    float32 \n","b16.conv1      2359808     16       [4, 512, 8, 8]      float32 \n","b16            -           16       [4, 512, 8, 8]      float32 \n","b8.skip        262144      16       [4, 512, 4, 4]      float32 \n","b8.conv0       2359808     16       [4, 512, 8, 8]      float32 \n","b8.conv1       2359808     16       [4, 512, 4, 4]      float32 \n","b8             -           16       [4, 512, 4, 4]      float32 \n","b4.mbstd       -           -        [4, 513, 4, 4]      float32 \n","b4.conv        2364416     16       [4, 512, 4, 4]      float32 \n","b4.fc          4194816     -        [4, 512]            float32 \n","b4.out         513         -        [4, 1]              float32 \n","---            ---         ---      ---                 ---     \n","Total          28982849    480      -                   -       \n","\n","Setting up augmentation...\n","Distributing across 1 GPUs...\n","Setting up training phases...\n","Exporting sample images...\n","Initializing logs...\n","Training for 25000 kimg...\n","\n","tick 0     kimg 0.0      time 1m 47s       sec/tick 8.2     sec/kimg 2042.22 maintenance 99.2   cpumem 7.23   gpumem 9.06   augment 0.000\n"]}]},{"cell_type":"markdown","metadata":{"id":"GwPrEVh5coPf"},"source":["### Mientras se está entrenando...\n","**Una vez que la celda anterior se esté ejecutando, ¡debería estar entrenando!**\n","\n","¡No cierre esta pestaña! Colab debe estar abierto y en funcionamiento para poder seguir entrenando.\n","\n","Cada 40 minutos más o menos debería añadirse una nueva línea a su salida, indicando que todavía está entrenando. Dependiendo de su configuración de `snapshot_count`, debería ver la carpeta de resultados (`/content/drive/MyDrive/colab-sg2-ada/stylegan2-ada/results`) en su carpeta de Google drive llenándose con ambas muestras (`fakesXXXXXx.jpg`) y *model weights* (pesos del modelo) (`network-snapshot-XXXXXX.pkl`). Vale la pena mirar las muestras mientras se entrena, pero no se preocupe demasiado por cada muestra individual.\n","\n","Una vez que Colab se apaga, puede volver a conectarse en el alojamiento y volver a ejecutar cada celda de arriba a abajo. Asegúrese de actualizar la ruta `resume_from` para continuar entrenando desde el último modelo."]}]}